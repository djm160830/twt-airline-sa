{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tweets_US_airline_SC",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMvPdugOMrolVs+Xf8AFFxF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/djm160830/twt-airline-sa/blob/master/Tweets_US_airline_SC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLKIGWJzCOgU"
      },
      "source": [
        "import sys\r\n",
        "import pandas as pd\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\r\n",
        "from sklearn import preprocessing\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.naive_bayes import MultinomialNB\r\n",
        "from sklearn import metrics\r\n",
        "import numpy as np\r\n",
        "from prettytable import PrettyTable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vD0irbhVCaHW"
      },
      "source": [
        "def read_tweets():\r\n",
        "  \"\"\"Reads Tweets from US Airline Sentiment dataset.\r\n",
        "\r\n",
        "  Returns:\r\n",
        "  DataFrame: Table containing Tweets, which airline the Tweet is referring to, and sentiment of Tweet\r\n",
        "\r\n",
        "  \"\"\"\r\n",
        "\treturn pd.read_csv(\"https://raw.githubusercontent.com/djm160830/twt-airline-sa/master/archive/Tweets.csv\", \r\n",
        "\t\tusecols=[\"airline_sentiment\", \"airline\", \"text\"])\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BddsmDPJU8r"
      },
      "source": [
        "Convert text to lowercase because: \r\n",
        "\r\n",
        "CountVectorizer() to convert each text document (Tweet) into a matrix of token counts \r\n",
        "\r\n",
        "TfidfTransformer() to weigh each feature name from CountVectorizer(). Selects "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N31qmJQ5CcGm"
      },
      "source": [
        "def preprocess(data):\r\n",
        "\t\"\"\"Converts words to lowercase, performs label encoding, train/test split, tf-idf scores\r\n",
        "\t\r\n",
        "\tParameters: \r\n",
        "\tdata (DataFrame): Table containing Tweets, which airline the Tweet is referring to, and sentiment of Tweet\r\n",
        "\t\r\n",
        "\tReturns:\r\n",
        "  counts (sparse matrix): Transformed count vectorized matrix of words from training dataset into a tf-idf representation\r\n",
        "  X_test_tf (sparse matrix): Transformed count vectorized matrix of words from test dataset into a tf-idf representation\r\n",
        "  y_train (Series): Label encoded target variable from training dataset\r\n",
        "  y_test (Series): Label encoded target variable from testing dataset\r\n",
        "  data['airline_sentiment'] (DataFrame): Table of label encoded sentiments\r\n",
        "  X_test (DataFrame): Raw feature variables\r\n",
        "\t\"\"\"\r\n",
        "\t# Convert text to lowercase \r\n",
        "\tfor column in data.columns:\r\n",
        "\t\tdata[column] = data[column].str.lower()\r\n",
        "\r\n",
        "\t# Categorize target variable\r\n",
        "\tle = preprocessing.LabelEncoder()\r\n",
        "\tdata['airline_sentiment'] = le.fit_transform(data['airline_sentiment'])\r\n",
        "\r\n",
        "\t# Split data into training and testing (10% testing) using train_test_split\r\n",
        "\tX_train, X_test, y_train, y_test = train_test_split(data.iloc[:, 1:], data.iloc[:, 0], test_size=0.10)\r\n",
        " \r\n",
        "\t# Transform training text using countvectorizer and tfidftransformer\r\n",
        "\t\"\"\"\r\n",
        "\tCountVectorizer(): Counts frequency of words\r\n",
        "\tTfidfTransformer(): Adjusts for the fact that some words appear more frequently in general (ex: 'we', 'the').\r\n",
        "\t\"\"\"\r\n",
        "\tcount_vect = CountVectorizer() \r\n",
        "\tcounts = count_vect.fit_transform(X_train['text']) # Learn the vocabulary dictionary\r\n",
        "\ttransformer = TfidfTransformer(use_idf=True)\r\n",
        "\tcounts = transformer.fit_transform(counts) \t\t\t# Learn IDF vector (global term weights), and transform a count matrix to a tf-idf representation\r\n",
        "\r\n",
        "\t# Process test data\r\n",
        "\tX_test_cv = count_vect.transform(X_test['text']) \r\n",
        "\tX_test_tf = transformer.transform(X_test_cv)\r\n",
        "\r\n",
        "\treturn counts, X_test_tf, y_train, y_test, data['airline_sentiment'], X_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xtci8q9mCd4j",
        "outputId": "f0989d18-7229-4e4e-e08f-9ef9cfb6aad1"
      },
      "source": [
        "t = PrettyTable(['ITERATION', 'ALPHA', 'FIT_PRIOR', 'TRAINING ACCURACY'])\r\n",
        "\r\n",
        "# Repeat process 5 times with different parameter choices a, fp (for laplace smoothing & fit_prior) and output the parameters and accuracy in a tabular format.\r\n",
        "for i, a in enumerate(np.linspace(1.25, 1.0e-10, 5)):\r\n",
        "  for _, fp in enumerate([True, True, True, False, False, False]):\r\n",
        "    df = read_tweets()\r\n",
        "\r\n",
        "    X_train, X_test, y_train, y_test, target_sentiment, X_test_raw = preprocess(df)\r\n",
        "\r\n",
        "    # Build a Multinomial Na√Øve Bayes (MNB) model using the training dataset\r\n",
        "    model = MultinomialNB(alpha=a, fit_prior=fp).fit(X_train, y_train)\r\n",
        "\r\n",
        "    # Apply model on test and output the accuracy\r\n",
        "    predicted = model.predict(X_test) \r\n",
        "    accuracy = model.score(X_train, y_train)\r\n",
        "    t.add_row([i+1, a, fp, accuracy])\r\n",
        "  if i!=4: t.add_row([' ', ' ', ' ', ' '])\t\r\n",
        "print(t)\r\n",
        "\r\n",
        "# Average sentiment of each airline, and which airline has the highest positive sentiment\r\n",
        "df['airline_sentiment'] = target_sentiment\r\n",
        "highest_sentiment = df.groupby('airline').agg(mean_sentiment=('airline_sentiment', 'mean')).sort_values(by='mean_sentiment', ascending=False)\r\n",
        "print(f'\\n{highest_sentiment}')\r\n",
        "print(f'\\nHighest positive sentiment: \\n{highest_sentiment[:1]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+----------------+-----------+--------------------+\n",
            "| ITERATION |     ALPHA      | FIT_PRIOR | TRAINING ACCURACY  |\n",
            "+-----------+----------------+-----------+--------------------+\n",
            "|     1     |      1.25      |    True   | 0.6918639951426837 |\n",
            "|     1     |      1.25      |    True   | 0.6940649666059502 |\n",
            "|     1     |      1.25      |    True   | 0.6929265330904675 |\n",
            "|     1     |      1.25      |   False   | 0.8047207043108683 |\n",
            "|     1     |      1.25      |   False   | 0.8044930176077717 |\n",
            "|     1     |      1.25      |   False   | 0.8047207043108683 |\n",
            "|           |                |           |                    |\n",
            "|     2     | 0.937500000025 |    True   | 0.7178202792956891 |\n",
            "|     2     | 0.937500000025 |    True   | 0.7186551305403764 |\n",
            "|     2     | 0.937500000025 |    True   | 0.7181997571341834 |\n",
            "|     2     | 0.937500000025 |   False   | 0.831056466302368  |\n",
            "|     2     | 0.937500000025 |   False   | 0.8313600485731634 |\n",
            "|     2     | 0.937500000025 |   False   | 0.8304493017607771 |\n",
            "|           |                |           |                    |\n",
            "|     3     | 0.62500000005  |    True   | 0.7641165755919854 |\n",
            "|     3     | 0.62500000005  |    True   | 0.7650273224043715 |\n",
            "|     3     | 0.62500000005  |    True   | 0.7625227686703097 |\n",
            "|     3     | 0.62500000005  |   False   | 0.8652094717668488 |\n",
            "|     3     | 0.62500000005  |   False   | 0.8636915604128719 |\n",
            "|     3     | 0.62500000005  |   False   | 0.8652094717668488 |\n",
            "|           |                |           |                    |\n",
            "|     4     | 0.312500000075 |    True   | 0.8263509411050395 |\n",
            "|     4     | 0.312500000075 |    True   | 0.8279447480267152 |\n",
            "|     4     | 0.312500000075 |    True   | 0.8268063145112325 |\n",
            "|     4     | 0.312500000075 |   False   | 0.8953400121432908 |\n",
            "|     4     | 0.312500000075 |   False   | 0.8943533697632058 |\n",
            "|     4     | 0.312500000075 |   False   | 0.8971615057680632 |\n",
            "|           |                |           |                    |\n",
            "|     5     |     1e-10      |    True   | 0.9150728597449909 |\n",
            "|     5     |     1e-10      |    True   | 0.9170461445051609 |\n",
            "|     5     |     1e-10      |    True   | 0.9148451730418944 |\n",
            "|     5     |     1e-10      |   False   | 0.9368548876745598 |\n",
            "|     5     |     1e-10      |   False   | 0.9373861566484517 |\n",
            "|     5     |     1e-10      |   False   | 0.9375379477838495 |\n",
            "+-----------+----------------+-----------+--------------------+\n",
            "\n",
            "                mean_sentiment\n",
            "airline                       \n",
            "virgin america        0.942460\n",
            "delta                 0.815032\n",
            "southwest             0.745455\n",
            "united                0.439822\n",
            "american              0.411381\n",
            "us airways            0.315482\n",
            "\n",
            "Highest positive sentiment: \n",
            "                mean_sentiment\n",
            "airline                       \n",
            "virgin america         0.94246\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-bzidM-k9Nv"
      },
      "source": [
        "According to the table, the model performed well with fit_prior=False, and **alpha**=1.0e-10. \r\n",
        "# What is alpha? \r\n",
        "Scikit-learn's documentation defines it as \"Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing).\" We want to use something like Laplace/Lidstone smoothing because frequency-based probability, like Tf-idf in this example, \"might introduce zeroes when multiplying probabilities, leading to a failure in preserving the information contributed by non-zero probabilities.\" We want to prevent a probability of zero if we know that something has the possibility (even the smallest possibility) of occurring, because \"this oversimplification is inaccurate and often unhelpful, particularly in probability-based machine learning techniques\". Laplace/Lidstone smoothing handles this problem of zero probability by adding a smoothing parameter, $ \\alpha $, to the probability of a single observation $x$ from a multinomial distribution with $ N $ trials and $k$ feature variables: \\begin{equation} \\frac{x_i+\\alpha}{N+\\alpha k} \\end{equation} In practice, a smaller value is typically chosen, as seen with this model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41gJxFz9-r9m"
      },
      "source": [
        "So how would this model predict the mean sentiment of US airlines, after having been trained on Tweets?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLxfkpgOIhKP",
        "outputId": "424772c9-20bb-4e7e-822d-59417e91c01c"
      },
      "source": [
        "X = X_test_raw\r\n",
        "p = X[\"airline\"].reset_index().join(pd.Series(predicted, name=\"sentiment\"))\r\n",
        "print(f'\\n{p.groupby(\"airline\").agg(mean_sentiment=(\"sentiment\", \"mean\")).sort_values(by=\"mean_sentiment\", ascending=False)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "                mean_sentiment\n",
            "airline                       \n",
            "virgin america        0.762712\n",
            "delta                 0.757991\n",
            "southwest             0.632000\n",
            "united                0.409214\n",
            "american              0.340351\n",
            "us airways            0.297872\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lusa2ExGXMA8"
      },
      "source": [
        "It looks like the model yielded similar results as it did with the training data. United and American were flipped around in this result, which makes sense given that their mean sentiments were also very close in the results produced with the training data. We can get a numerical representation of how accurate the model was with the test data using metrics.accuracy_score:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLT7KqWXYOY-",
        "outputId": "7fdd1d80-1928-4052-ed47-842a6b17c9c0"
      },
      "source": [
        "metrics.accuracy_score(y_test, predicted)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7151639344262295"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 304
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zjRYw7cf_Ij"
      },
      "source": [
        "# When Laplace smoothing falls short\r\n"
      ]
    }
  ]
}